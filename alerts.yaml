apiVersion: "nais.io/v1"
kind: "Alert"
metadata:
  name: teamsykmelding-pik-alerts
  namespace: teamsykmelding
  labels:
    team: teamsykmelding
spec:
  receivers:
    slack:
      channel: '#sykmelding-alerts'
      #prependText: '<!here> | '
  alerts:
    - alert: teamsykmelding-pik er nede
      expr: up{app="teamsykmelding-pik", job="kubernetes-pods"} == 0
      for: 5m
      description: "App {{ $labels.app }} er nede i namespace {{ $labels.kubernetes_namespace }}"
      action: "`kubectl describe pod {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for events, og `kubectl logs {{ $labels.kubernetes_pod_name }} -n {{ $labels.kubernetes_namespace }}` for logger"
    - alert: teamsykmelding-pik-kafka-offset
      expr: kafka_consumergroup_group_sum_lag{group=~"teamsykmelding-pik"} > 100
      for: 1m
      description: "teamsykmelding-pik kafka-lag er høyt"
      action: "Sjekk om teamsykmelding-pik leser fra kafka"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: teamsykmelding-pik error-logging
      expr: sum(rate(logd_messages_total{log_app=~"teamsykmelding-pik",log_level="Error",job="kubernetes-pods"}[5m])) by (log_app) > 0
      for: 5m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk hvorfor {{ $labels.log_app }} har logget Error i løpet av de siste 5 minuttene"
      sla: respond within 1h, during office hours
      severity: danger